---
---

## Generalized Linear Models

Generalized linear models can be fit in R using the `glm` function, which is a part of base R. The `formula` syntax in the `lm` and `glm` functions are the same, but an addional argument, `family`, is used in `glm` to specfiies the error distribution. In `lm` the distribution is assumed to be a normal or gaussian. Additionally, the `lm` function treats the response variable as numeric---the `glm` function
lifts this restriction and others. 

===

### GLM Families

The `family` argument determines the family of probability distributions in
which the response variable belongs. A key difference between families is the
data type and range.

===

| Family             | Data Type | (*default*) link functions             |
|--------------------|-----------|----------------------------------------|
| `gaussian`         | `double`  | *identity*, log, inverse               |
| `binomial`         | `boolean` | *logit*, probit, cauchit, log, cloglog |
| `poisson`          | `integer` | *log*, identity, sqrt                  |
| `Gamma`            | `double`  | *inverse*, identity, log               |
| `inverse.gaussian` | `double`  | *"1/mu^2"*, inverse, identity, log     |

===

The previous model fit with `lm` is (almost) identical to a model fit using
`glm` with the Gaussian family and identity link.

```{r, handout = 0}
fit <- glm(log(WAGP) ~ SCHL,
    family = gaussian,
    data = pums)
```

===

```{r}
summary(fit)
```

===

Question
: Should the coefficient estimates between this Gaussian `glm()` and the previous
`lm()` be different?

Answer
: {:.fragment} It's possible. The `lm()` function uses a different (less
general) fitting procedure than the `glm()` function, which uses IWLS. But with
64 bits used to store very precise numbers, it's rare to encounter an noticeable
difference in the optimum.

===

### Logistic Regression

Calling `glm` with `family = binomial` using the default "logit" link performs
logistic regression. We can use this to model the relationship between sex and wages as an example, although this is not a meaningful statistical test.

```{r, handout = 0}
fit <- glm(SEX ~ WAGP,
  family = binomial,
  data = pums)
```

===

Interpretting the coefficients in the summary is not obvious, but always works
the same way. In the `pums` table, where men are coded as `1`, the `levels`
function shows that `2`, or women, are the first level. When using the binomial
family, the first level of the factor is consider 0 or "failure", and all remaining
levels (typically just one) are considered 1 or "success". The predicted value, the linear combination of variables and coeficients, is the log odds of "success".
{:.notes}

The positive coefficient on `WAGP` implies that a higher wage tilts the
prediction towards men.

```{r}
summary(fit)
```

===

The always popular $$R^2$$ indicator for goodness-of-fit is absent from the
summary of a `glm` result, as is the defult F-Test of the model's significance.

The developers of `glm`, detecting an increase in user sophistication, are
leaving more of the model assessment up to you. The "null deviance" and
"residual deviance" provide most of the information we need. The `?anova.glm`
function allows us to apply the F-test on the deviance change, although there is
a better alternative.
{:.notes}

```{r, handout = 0}
anova(fit, update(fit, SEX ~ 1), test = 'Chisq')
```

===

### Observation Weights

Both the `lm()` and `glm()` function allow a vector of weights the same length
as the response. Weights can be necessary for logistic regression, depending on
the format of the data. The `binomial` family `glm()` works with three different
response variable formats.

1. `factor` with only, or coerced to, two levels (binary)
1. `matrix` with two columns for the count of "successes" and "failures"
1. `numeric` proprtion of "successes" out of `weights` trials

Depending on your model, these three formats are not necesarilly
interchangeable.
{:.notes}