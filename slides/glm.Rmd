---
---

## Generalized Linear Models

In linear models, like those fit with `lm`, the distribution of the response variable is assumed to be a normal or gaussian. However, in many cases this assumption is not appropriate. *Generalized* linear models allow for other distributions for the response variable, such as binomial or exponential distributions. In generalized linear models, a *link* function is used to transform the response variable. This causes the response variable to follow a more linear relationship with the predictor variables. 

===

Generalized linear models can be fit in R using the `glm` function, which is a part of base R. The `formula` syntax in the `lm` and `glm` functions are the same, but an additional argument, `family`, is used in `glm` to specify the distribution. 

The `family` argument determines the family of probability distributions in
which the response variable belongs.

===

| Family             | Data Type | (*default*) link functions             |
|--------------------|-----------|----------------------------------------|
| `gaussian`         | `double`  | *identity*, log, inverse               |
| `binomial`         | `boolean` | *logit*, probit, cauchit, log, cloglog |
| `poisson`          | `integer` | *log*, identity, sqrt                  |
| `Gamma`            | `double`  | *inverse*, identity, log               |
| `inverse.gaussian` | `double`  | *"1/mu^2"*, inverse, identity, log     |



===

The previous model fit with `lm` is (almost) identical to a model fit using
`glm` with the Gaussian family and identity link.

```{r, handout = 0}
fit <- glm(log(WAGP) ~ SCHL,
    family = gaussian,
    data = pums)
```

===

```{r}
summary(fit)
```

===

Question
: Should the coefficient estimates between this Gaussian `glm()` and the previous
`lm()` be different?

Answer
: {:.fragment} It's possible. The `lm()` function uses a different (less
general) fitting procedure than the `glm()` function, which uses IWLS. But with
64 bits used to store very precise numbers, it's rare to encounter an noticeable
difference in the optimum.

===

### Logistic Regression

Calling `glm` with `family = binomial` using the default "logit" link performs
logistic regression. We can use this to model the relationship between sex and wages as an example, although this is not a meaningful statistical test.

```{r, handout = 0}
fit <- glm(SEX ~ WAGP,
  family = binomial,
  data = pums)
```

===

Interpreting the coefficients in the summary is not obvious, but always works
the same way. In the `pums` table, where men are coded as `1`, the `levels`
function shows that `2`, or women, are the first level. When using the binomial
family, the first level of the factor is consider 0 or "failure", and all remaining
levels (typically just one) are considered 1 or "success". The predicted value, the linear combination of variables and coefficients, is the log odds of "success".
{:.notes}

The positive coefficient on `WAGP` implies that a higher wage tilts the
prediction towards men.

```{r}
summary(fit)
```

===

The always popular $$R^2$$ indicator for goodness-of-fit is absent from the
summary of a `glm` result, as is the defult F-Test of the model's significance.
The developers of `glm`, detecting an increase in user sophistication, are
leaving more of the model assessment up to you. The "null deviance" and
"residual deviance" provide most of the information we need. To test the fit of a generalized linear model, it is best to compare the deviance of model to the "null model," which is the model without predictor variables. Therefore, the null model only fits the intercept. 
{:.notes}

Use a Chi-squared test to see if the deviance in your model is lower than the null model.

```{r, handout = 0}
anova(fit, update(fit, SEX ~ 1), test = 'Chisq')
```
